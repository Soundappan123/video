{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistral_common\n",
      "  Using cached mistral_common-1.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.21.1 (from mistral_common)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting numpy>=1.25 (from mistral_common)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow<11.0.0,>=10.3.0 (from mistral_common)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pydantic<3.0.0,>=2.6.1 (from mistral_common)\n",
      "  Using cached pydantic-2.10.1-py3-none-any.whl.metadata (169 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /root/miniconda3/lib/python3.12/site-packages (from mistral_common) (2.32.2)\n",
      "Collecting sentencepiece==0.2.0 (from mistral_common)\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral_common)\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /root/miniconda3/lib/python3.12/site-packages (from mistral_common) (4.12.2)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.21.1->mistral_common)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.21.1->mistral_common)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.21.1->mistral_common)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.21.1->mistral_common)\n",
      "  Using cached rpds_py-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.6.1->mistral_common)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3.0.0,>=2.6.1->mistral_common)\n",
      "  Using cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral_common) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral_common) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral_common) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral_common) (2024.8.30)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.8.0,>=0.7.0->mistral_common)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Using cached mistral_common-1.5.1-py3-none-any.whl (6.5 MB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached pydantic-2.10.1-py3-none-any.whl (455 kB)\n",
      "Using cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached rpds_py-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Installing collected packages: sentencepiece, rpds-py, regex, pydantic-core, pillow, numpy, attrs, annotated-types, tiktoken, referencing, pydantic, jsonschema-specifications, jsonschema, mistral_common\n",
      "Successfully installed annotated-types-0.7.0 attrs-24.2.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 mistral_common-1.5.1 numpy-2.1.3 pillow-10.4.0 pydantic-2.10.1 pydantic-core-2.27.1 referencing-0.35.1 regex-2024.11.6 rpds-py-0.21.0 sentencepiece-0.2.0 tiktoken-0.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting vllm\n",
      "  Using cached vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.12/site-packages (from vllm) (6.1.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.12/site-packages (from vllm) (0.2.0)\n",
      "Collecting numpy<2.0.0 (from vllm)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.12/site-packages (from vllm) (4.66.4)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting transformers>=4.45.2 (from vllm)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tokenizers>=0.19.1 (from vllm)\n",
      "  Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting protobuf (from vllm)\n",
      "  Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting aiohttp (from vllm)\n",
      "  Using cached aiohttp-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting openai>=1.45.0 (from vllm)\n",
      "  Using cached openai-1.55.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn[standard] (from vllm)\n",
      "  Using cached uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pydantic>=2.9 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (2.10.1)\n",
      "Requirement already satisfied: pillow in /root/miniconda3/lib/python3.12/site-packages (from vllm) (10.4.0)\n",
      "Collecting prometheus-client>=0.18.0 (from vllm)\n",
      "  Using cached prometheus_client-0.21.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (0.7.0)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm)\n",
      "  Using cached lm_format_enforcer-0.10.9-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines<0.1,>=0.0.43 (from vllm)\n",
      "  Using cached outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (4.12.2)\n",
      "Collecting filelock>=3.10.4 (from vllm)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Using cached partial_json_parser-0.2.1.1.post4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyzmq in /root/miniconda3/lib/python3.12/site-packages (from vllm) (25.1.2)\n",
      "Collecting msgspec (from vllm)\n",
      "  Using cached msgspec-0.18.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.10.0 (from vllm)\n",
      "  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib-metadata in /root/miniconda3/lib/python3.12/site-packages (from vllm) (8.5.0)\n",
      "Requirement already satisfied: mistral-common>=1.5.0 in /root/miniconda3/lib/python3.12/site-packages (from mistral-common[opencv]>=1.5.0->vllm) (1.5.1)\n",
      "Collecting pyyaml (from vllm)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting compressed-tensors==0.8.0 (from vllm)\n",
      "  Using cached compressed_tensors-0.8.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting ray>=2.9 (from vllm)\n",
      "  Using cached ray-2.39.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting nvidia-ml-py>=12.560.30 (from vllm)\n",
      "  Using cached nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting torch==2.5.1 (from vllm)\n",
      "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.20.1 (from vllm)\n",
      "  Using cached torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.28.post3 (from vllm)\n",
      "  Using cached xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: six>=1.16.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (1.16.0)\n",
      "Collecting setuptools>=74.1.1 (from vllm)\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from vllm)\n",
      "  Using cached fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting networkx (from torch==2.5.1->vllm)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.5.1->vllm)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch==2.5.1->vllm)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->vllm)\n",
      "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1->vllm)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.5.1->vllm)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer<0.11,>=0.10.9->vllm)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (23.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /root/miniconda3/lib/python3.12/site-packages (from mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm) (4.23.0)\n",
      "Collecting opencv-python-headless<5.0.0,>=4.0.0 (from mistral-common[opencv]>=1.5.0->vllm)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=1.45.0->vllm)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /root/miniconda3/lib/python3.12/site-packages (from openai>=1.45.0->vllm) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.45.0->vllm)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.45.0->vllm)\n",
      "  Using cached jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.45.0->vllm)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting lark (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nest-asyncio in /root/miniconda3/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm) (1.6.0)\n",
      "Collecting cloudpickle (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting numba (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached numba-0.60.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: referencing in /root/miniconda3/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm) (0.35.1)\n",
      "Collecting datasets (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/lib/python3.12/site-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /root/miniconda3/lib/python3.12/site-packages (from pydantic>=2.9->vllm) (2.27.1)\n",
      "Collecting click>=7.0 (from ray>=2.9->vllm)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.9->vllm)\n",
      "  Using cached msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting aiosignal (from ray>=2.9->vllm)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting frozenlist (from ray>=2.9->vllm)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /root/miniconda3/lib/python3.12/site-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.19.1->vllm)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.45.2->vllm)\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->vllm)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->vllm) (24.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->vllm)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->vllm)\n",
      "  Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->vllm)\n",
      "  Using cached yarl-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/miniconda3/lib/python3.12/site-packages (from importlib-metadata->vllm) (3.21.0)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]->vllm)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
      "  Using cached uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Using cached watchfiles-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]->vllm)\n",
      "  Using cached websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.45.0->vllm)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm) (0.21.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.5.1->vllm)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.5.1->vllm)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached llvmlite-0.43.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl (198.9 MB)\n",
      "Using cached compressed_tensors-0.8.0-py3-none-any.whl (86 kB)\n",
      "Using cached gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "Using cached xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "Using cached fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached lm_format_enforcer-0.10.9-py3-none-any.whl (43 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "Using cached openai-1.55.0-py3-none-any.whl (389 kB)\n",
      "Using cached outlines-0.0.46-py3-none-any.whl (101 kB)\n",
      "Using cached prometheus_client-0.21.0-py3-none-any.whl (54 kB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached ray-2.39.0-cp312-cp312-manylinux2014_x86_64.whl (66.4 MB)\n",
      "Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached aiohttp-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached msgspec-0.18.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Using cached partial_json_parser-0.2.1.1.post4-py3-none-any.whl (9.9 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Using cached uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Using cached watchfiles-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "Using cached websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
      "Using cached yarl-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached numba-0.60.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Using cached pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached llvmlite-0.43.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, pyairports, py-cpuinfo, nvidia-ml-py, mpmath, xxhash, websockets, uvloop, tzdata, sympy, sniffio, setuptools, safetensors, pyyaml, python-dotenv, pycountry, pyarrow, protobuf, propcache, prometheus-client, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgspec, msgpack, MarkupSafe, llvmlite, lark, jiter, interegular, httptools, h11, fsspec, frozenlist, filelock, einops, diskcache, dill, cloudpickle, click, aiohappyeyeballs, yarl, uvicorn, triton, pandas, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, multiprocess, jinja2, huggingface-hub, httpcore, gguf, anyio, aiosignal, watchfiles, tokenizers, starlette, nvidia-cusolver-cu12, lm-format-enforcer, httpx, aiohttp, transformers, torch, ray, prometheus-fastapi-instrumentator, openai, fastapi, xformers, torchvision, datasets, compressed-tensors, outlines, vllm\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.5.1\n",
      "    Uninstalling setuptools-69.5.1:\n",
      "      Successfully uninstalled setuptools-69.5.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed MarkupSafe-3.0.2 aiohappyeyeballs-2.4.3 aiohttp-3.11.7 aiosignal-1.3.1 anyio-4.6.2.post1 click-8.1.7 cloudpickle-3.1.0 compressed-tensors-0.8.0 datasets-3.1.0 dill-0.3.8 diskcache-5.6.3 einops-0.8.0 fastapi-0.115.5 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 gguf-0.10.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.27.2 huggingface-hub-0.26.2 interegular-0.3.3 jinja2-3.1.4 jiter-0.7.1 lark-1.2.2 llvmlite-0.43.0 lm-format-enforcer-0.10.9 mpmath-1.3.0 msgpack-1.1.0 msgspec-0.18.6 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 numba-0.60.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py-12.560.30 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.55.0 opencv-python-headless-4.10.0.84 outlines-0.0.46 pandas-2.2.3 partial-json-parser-0.2.1.1.post4 prometheus-client-0.21.0 prometheus-fastapi-instrumentator-7.0.0 propcache-0.2.0 protobuf-5.28.3 py-cpuinfo-9.0.0 pyairports-2.1.1 pyarrow-18.0.0 pycountry-24.6.1 python-dotenv-1.0.1 pytz-2024.2 pyyaml-6.0.2 ray-2.39.0 safetensors-0.4.5 setuptools-75.6.0 sniffio-1.3.1 starlette-0.41.3 sympy-1.13.1 tokenizers-0.20.3 torch-2.5.1 torchvision-0.20.1 transformers-4.46.3 triton-3.1.0 tzdata-2024.2 uvicorn-0.32.1 uvloop-0.21.0 vllm-0.6.4.post1 watchfiles-0.24.0 websockets-14.1 xformers-0.0.28.post3 xxhash-3.5.0 yarl-1.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade mistral_common\n",
    "!pip install --upgrade vllm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-22 09:03:48,968\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from huggingface_hub import login\n",
    "import base64\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Set up Hugging Face token\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "login(token=os.environ['HF_TOKEN'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 09:04:16 config.py:1861] Downcasting torch.float32 to torch.float16.\n",
      "INFO 11-22 09:04:16 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='mistralai/Pixtral-12B-2409', speculative_config=None, tokenizer='mistralai/Pixtral-12B-2409', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Pixtral-12B-2409, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:15<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 09:04:19 model_runner.py:1072] Starting to load model mistralai/Pixtral-12B-2409...\n",
      "INFO 11-22 09:04:20 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-22 09:04:20 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [03:47<00:00, 227.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [03:47<00:00, 227.53s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 09:08:26 model_runner.py:1077] Loading model weights took 23.6239 GB\n",
      "INFO 11-22 09:08:31 worker.py:232] Memory profiling results: total_gpu_memory=47.54GiB initial_memory_usage=23.98GiB peak_torch_memory=24.76GiB memory_usage_post_profile=24.02GiB non_torch_memory=0.36GiB kv_cache_size=17.67GiB gpu_memory_utilization=0.90\n",
      "INFO 11-22 09:08:31 gpu_executor.py:113] # GPU blocks: 7236, # CPU blocks: 1638\n",
      "INFO 11-22 09:08:31 gpu_executor.py:117] Maximum concurrency for 10000 tokens per request: 11.58x\n",
      "INFO 11-22 09:08:37 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-22 09:08:37 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-22 09:09:01 model_runner.py:1518] Graph capturing finished in 24 secs, took 0.27 GiB\n"
     ]
    }
   ],
   "source": [
    "# Define Pixtral model details\n",
    "model_name = \"mistralai/Pixtral-12B-2409\"\n",
    "sampling_params = SamplingParams(max_tokens=8192, temperature=0.7)\n",
    "llm = LLM(model=model_name, tokenizer_mode=\"mistral\", max_model_len=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_base64(image_path):\n",
    "    \"\"\"Encodes an image to a base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of image paths\n",
    "image_paths = [\n",
    "    \"/home/1731514749518Screenshot2024-11-12143232png1731494987614 1.png\",\n",
    "    \"/home/image1.png\"\n",
    "]\n",
    "\n",
    "# Prepare images in Base64 format for Pixtral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "for image_path in image_paths:\n",
    "    img_base64 = encode_image_base64(image_path)\n",
    "    # Image data formatted for API usage\n",
    "    content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_chat(single_message, sampling_params):\n",
    "    # Simulating output as if Pixtral processed it\n",
    "    return {\"choices\": [{\"message\": {\"content\": \"Extracted text from image\"}}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images...\n",
      "INFO 11-22 09:21:33 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.37s/it, est. speed input: 115.38 toks/s, output: 26.23 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Raw output for Image 1:\n",
      "[RequestOutput(request_id=2, prompt=None, prompt_token_ids=[1, 3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 13, 4], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='Certainly! Here is a detailed summary of the document provided:\\n\\n---\\n\\n**Electrodiagnostic Medicine Consultation**\\n\\n**REVIEW OF SYSTEMS**\\n\\nThe document is titled \"Electrodiagnostic Medicine Consultation\" and presents a review of various systems in a patient\\'s medical assessment. Each system is evaluated for the presence or absence of specific symptoms or conditions. The evaluation is structured with \"Yes\" or \"No\" responses for each item.\\n\\n1. **Visual Changes:** Indicates whether there are any changes in vision.\\n2. **Dizziness/Falling/Difficulty with Balance:** Assesses for any balance issues.\\n3. **Vertigo/Fainting Spells:** Evaluates for symptoms of vertigo or fainting.\\n4. **Difficulty Swallowing:** Checks for swallowing difficulties.\\n5. **Speech Problems:** Looks for any problems with speech.\\n6. **Decreased Facial Sensation:** Determines if there is a reduction in facial sensation.\\n7. **Muscle Paralysis/Atrophy/Muscle Weakness:** Assesses for muscle-related issues.\\n8. **Involuntary Movements/Tremors:** Evaluates for involuntary movements or tremors.\\n9. **Numbness/Tingling/Burning Sensation:** Checks for sensory issues.\\n10. **Loss of Sensation:** Assesses for any loss of sensation.\\n11. **Pain/Muscle Pain/Myalgia/Back Pain/Neck Pain:** Evaluates for muscle or joint pain.\\n12. **Joint Pain/Swelling/Stiffness:** Assesses for joint-related issues.\\n13. **Range of Motion:** Checks for limitations in range of motion.\\n14. **Muscle Cramps/Leg Cramps/Muscle Spasms:** Evaluates for muscle cramps or spasms.\\n15. **Loss of Strength/Difficulty with Grip:** Assesses for loss of strength or grip difficulties.\\n16. **Bladder/Bowel Incontinence:** Checks for bladder or bowel control issues.\\n17. **Memory Loss/Difficulty Concentrating:** Evaluates for memory or concentration issues.\\n18. **Depression:** Assesses for symptoms of depression.\\n19. **Anxiety:** Checks for symptoms of anxiety.\\n20. **Seizures:** Evaluates for seizures.\\n21. **Headaches:** Assesses for headaches.\\n22. **Sleep Disturbances/Insomnia:** Checks for sleep disturbances.\\n23. **Night Sweats/Fever/Chills:** Evaluates for night sweats or fevers.\\n24. **Weight Loss:** Assesses for unintentional weight loss.\\n25. **Weight Gain Recently:** Checks for recent weight gain.\\n26. **Shortness of Breath/Cough:** Evaluates for respiratory issues.\\n27. **Constipation:** Assesses for constipation.\\n28. **Diarrhea:** Evaluates for diarrhea.\\n\\n---\\n\\nThis structured review helps in identifying and documenting various symptoms that could be relevant for an electrodiagnostic medicine consultation, ensuring a comprehensive assessment of the patient\\'s condition.', token_ids=(117991, 1405, 1033, 9380, 1395, 1261, 15115, 17793, 1307, 1278, 4440, 5662, 2100, 16166, 1438, 60525, 1291, 100889, 13203, 27500, 51978, 1370, 3448, 1438, 3609, 111111, 16622, 45415, 3074, 1069, 7985, 3448, 1784, 4440, 1395, 29818, 1429, 60525, 1291, 100889, 13203, 27500, 51978, 1370, 1034, 1321, 20618, 1261, 6369, 1307, 5561, 6467, 1294, 1261, 6482, 1681, 9371, 14013, 1046, 11796, 2663, 1395, 13985, 1394, 1278, 7361, 1505, 13730, 1307, 4811, 11929, 1505, 5481, 1046, 1531, 13664, 1395, 37253, 1454, 1429, 16860, 1034, 1505, 1429, 4753, 1034, 11688, 1394, 2744, 5610, 1338, 1049, 1046, 1603, 39441, 46344, 3795, 77291, 2130, 5627, 2156, 1584, 2258, 5266, 1294, 15198, 1626, 1050, 1046, 1603, 1068, 13706, 4555, 28546, 32575, 18392, 4940, 83197, 1454, 74980, 3795, 4727, 25016, 1394, 2258, 13864, 8520, 1626, 1051, 1046, 1603, 27943, 7378, 28546, 1493, 3333, 3434, 9327, 3795, 110361, 2130, 1394, 11929, 1307, 7188, 7378, 1505, 1284, 1493, 3333, 1626, 1052, 1046, 1603, 39811, 83197, 6767, 16917, 1302, 3795, 100507, 1394, 109369, 25618, 1626, 1053, 1046, 1603, 116428, 65566, 3795, 88557, 1394, 2258, 7638, 1454, 16181, 1626, 1054, 1046, 1603, 3761, 92408, 21278, 1551, 55489, 1370, 3795, 40398, 2783, 1693, 2156, 1395, 1261, 10550, 1294, 39572, 38261, 1626, 1055, 1046, 1603, 20948, 3911, 84548, 3627, 1047, 5629, 26054, 17124, 1374, 3911, 108440, 3651, 3795, 4727, 25016, 1394, 13612, 12662, 8520, 1626, 1056, 1046, 1603, 1785, 3191, 3458, 1760, 22310, 4451, 27501, 3157, 1839, 3795, 110361, 2130, 1394, 89640, 1760, 20775, 1505, 19617, 1839, 1626, 1057, 1046, 1603, 1078, 5430, 3651, 27501, 1302, 4604, 23015, 1645, 1302, 55489, 1370, 3795, 100507, 1394, 33655, 8520, 1626, 1049, 1048, 1046, 1603, 77641, 1307, 55489, 1370, 3795, 4727, 25016, 1394, 2258, 6410, 1307, 38261, 1626, 1049, 1049, 1046, 1603, 111763, 17124, 1374, 3911, 39542, 67437, 1279, 21535, 1047, 9824, 39542, 1047, 11993, 1800, 39542, 3795, 110361, 2130, 1394, 13612, 1505, 12201, 6890, 1626, 1049, 1050, 1046, 1603, 87928, 39542, 14327, 112815, 1047, 1989, 4940, 3651, 3795, 4727, 25016, 1394, 12201, 12662, 8520, 1626, 1049, 1051, 1046, 1603, 15121, 1307, 48876, 3795, 100507, 1394, 21575, 1294, 4521, 1307, 12968, 1626, 1049, 1052, 1046, 1603, 20948, 3911, 1359, 5463, 1822, 15288, 2055, 1359, 5463, 1822, 17124, 1374, 3911, 3434, 1288, 2943, 3795, 110361, 2130, 1394, 13612, 2989, 23248, 1505, 120087, 2943, 1626, 1049, 1053, 1046, 1603, 77641, 1307, 77036, 18392, 4940, 83197, 1454, 1461, 8249, 3795, 4727, 25016, 1394, 6410, 1307, 8646, 1505, 33885, 25618, 1626, 1049, 1054, 1046, 1603, 5855, 31273, 23015, 66132, 1656, 119449, 3795, 100507, 1394, 42206, 1505, 55404, 3119, 8520, 1626, 1049, 1055, 1046, 1603, 28045, 44376, 18392, 4940, 83197, 78928, 33048, 3795, 110361, 2130, 1394, 6628, 1505, 8186, 8520, 1626, 1049, 1056, 1046, 1603, 3761, 3329, 3795, 4727, 25016, 1394, 11929, 1307, 18596, 1626, 1049, 1057, 1046, 1603, 4051, 97892, 3795, 100507, 1394, 11929, 1307, 21896, 1626, 1050, 1048, 1046, 1603, 3201, 1518, 2456, 3795, 110361, 2130, 1394, 61311, 1626, 1050, 1049, 1046, 1603, 17287, 20964, 3795, 4727, 25016, 1394, 108671, 1626, 1050, 1050, 1046, 1603, 73936, 8542, 12436, 4446, 1047, 17653, 84873, 3795, 100507, 1394, 10301, 61607, 1626, 1050, 1051, 1046, 1603, 59110, 82679, 2476, 28546, 2686, 1047, 2814, 6671, 3795, 110361, 2130, 1394, 6757, 10570, 2476, 1505, 2337, 3085, 1626, 1050, 1052, 1046, 1603, 18453, 44376, 3795, 4727, 25016, 1394, 69785, 1297, 2957, 5954, 6410, 1626, 1050, 1053, 1046, 1603, 18453, 100585, 44229, 3795, 100507, 1394, 8531, 5954, 12542, 1626, 1050, 1054, 1046, 1603, 28897, 3651, 1307, 117495, 14787, 2620, 3795, 110361, 2130, 1394, 23730, 8520, 1626, 1050, 1055, 1046, 1603, 19217, 76475, 3795, 4727, 25016, 1394, 2018, 76475, 1626, 1050, 1056, 1046, 1603, 20609, 3638, 42001, 3795, 110361, 2130, 1394, 75017, 1338, 16166, 4380, 37253, 6369, 16348, 1294, 25330, 1321, 129341, 5561, 11929, 1455, 2481, 1402, 11157, 1394, 1420, 7798, 100889, 13203, 21527, 46116, 1044, 40227, 1261, 23157, 14013, 1307, 1278, 6482, 1681, 6570, 1046, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1732267293.8093266, last_token_time=1732267293.8093266, first_scheduled_time=1732267293.814957, first_token_time=1732267294.640673, time_in_queue=0.0056302547454833984, finished_time=1732267318.142089, scheduler_time=0.07440024614334106, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0)]\n",
      "\n",
      "INFO 11-22 09:21:58 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:30<00:00, 30.61s/it, est. speed input: 55.31 toks/s, output: 26.89 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Raw output for Image 2:\n",
      "[RequestOutput(request_id=3, prompt=None, prompt_token_ids=[1, 3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 13, 4], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='Sure, here is a detailed summary of the provided document, which appears to be a medical bill statement from Quantum Radiology PC for a patient named Adrian Adams:\\n\\n**Document Details:**\\n- **Generated Date:** 4/19/2022\\n- **Date Range Start:** 11/4/2021\\n- **Date Range End:** 4/19/2022\\n- **Responsible Party:** Adrian Adams\\n- **Account Number:** 885823-QUINT\\n\\n**Billing Information for Adrian Adams:**\\n1. **Date:** 12/21/2021\\n   - **Physician:** Jay P Patel\\n   - **Service:** CT UPPER EXTREMITY W/O DYE\\n   - **Amount:** $261.00\\n   - **Insurance Payment:** $56.17\\n   - **Adjustments:** $167.39\\n   - **Patient Payment:** $0.00\\n   - **Remaining Balance:** $37.44\\n\\n2. **Date:** 11/04/2021\\n   - **Physician:** Kerbsun G Crockett\\n   - **Service:** CT NECK SPINE W/O DYE\\n   - **Amount:** $729.00\\n   - **Insurance Payment:** $56.17\\n   - **Adjustments:** $185.39\\n   - **Patient Payment:** $0.00\\n   - **Remaining Balance:** $37.44\\n   - **Note:** **SENT TO COLLECTION AGENCY**\\n\\n3. **Date:** 11/04/2021\\n   - **Physician:** Jay G Patel\\n   - **Service:** X-RAY EXAM OF SHOULDER\\n   - **Amount:** $44.00\\n   - **Insurance Payment:** $10.75\\n   - **Adjustments:** $26.08\\n   - **Patient Payment:** $0.00\\n   - **Remaining Balance:** $7.17\\n\\n4. **Date:** 11/04/2021\\n   - **Physician:** Kerbsun G Crockett\\n   - **Service:** MED SERV 10PM-6AM 24 HR FAC\\n   - **Amount:** $138.00\\n   - **Insurance Payment:** $0.00\\n   - **Adjustments:** $138.00\\n   - **Patient Payment:** $0.00\\n   - **Remaining Balance:** $0.00\\n   - **Note:** **DENIED BY INSURANCE**\\n\\n5. **Date:** 11/04/2021\\n   - **Physician:** Jay G Patel\\n   - **Service:** X-RAY EXAM OF SHOULDER\\n   - **Amount:** $44.00\\n   - **Insurance Payment:** $10.75\\n   - **Adjustments:** $26.08\\n   - **Patient Payment:** $0.00\\n   - **Remaining Balance:** $7.17\\n\\n**Totals:**\\n- **Total Amount:** $766.00\\n- **Total Insurance Payments:** $133.84\\n- **Total Adjustments:** $542.94\\n- **Total Patient Payments:** $0.00\\n- **Remaining Balance:** $89.22\\n\\nThis document provides a breakdown of services rendered, payments from insurance, adjustments, patient payments, and remaining balances for the specified date range. There is a note indicating that one service was sent to a collection agency, and another was denied by insurance.', token_ids=(69957, 1044, 3226, 1395, 1261, 15115, 17793, 1307, 1278, 5662, 4440, 1044, 1799, 10249, 1317, 1402, 1261, 9371, 11440, 10674, 1562, 91730, 42554, 3678, 8001, 1394, 1261, 6482, 8876, 41520, 28055, 2100, 1438, 16060, 33537, 24185, 1045, 1603, 47964, 7600, 3795, 1032, 1052, 1047, 1049, 1057, 1047, 1050, 1048, 1050, 1050, 1010, 1045, 1603, 4638, 31414, 13336, 3795, 1032, 1049, 1049, 1047, 1052, 1047, 1050, 1048, 1050, 1049, 1010, 1045, 1603, 4638, 31414, 7114, 3795, 1032, 1052, 1047, 1049, 1057, 1047, 1050, 1048, 1050, 1050, 1010, 1045, 1603, 3065, 4288, 2416, 9296, 3795, 41520, 28055, 1010, 1045, 1603, 13256, 13920, 3795, 1032, 1056, 1056, 1053, 1056, 1050, 1051, 1045, 27289, 18320, 1267, 1438, 127336, 15696, 1394, 41520, 28055, 24185, 1049, 1046, 1603, 4638, 3795, 1032, 1049, 1050, 1047, 1050, 1049, 1047, 1050, 1048, 1050, 1049, 1010, 1256, 1462, 1603, 41676, 14044, 3795, 25773, 1390, 110611, 1010, 1256, 1462, 1603, 4271, 3795, 17134, 1608, 11388, 2794, 17631, 1084, 83886, 23921, 1488, 33338, 1407, 1089, 1069, 1010, 1256, 1462, 1603, 19745, 3795, 1659, 1050, 1054, 1049, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 17653, 13213, 56613, 3795, 1659, 1053, 1054, 1046, 1049, 1055, 1010, 1256, 1462, 1603, 103266, 3033, 3795, 1659, 1049, 1054, 1055, 1046, 1051, 1057, 1010, 1256, 1462, 1603, 54683, 56613, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 104008, 74980, 3795, 1659, 1051, 1055, 1046, 1052, 1052, 1267, 1050, 1046, 1603, 4638, 3795, 1032, 1049, 1049, 1047, 1048, 1052, 1047, 1050, 1048, 1050, 1049, 1010, 1256, 1462, 1603, 41676, 14044, 3795, 18755, 5136, 1384, 1461, 14846, 1800, 2928, 1010, 1256, 1462, 1603, 4271, 3795, 17134, 117723, 1075, 15180, 17779, 1488, 33338, 1407, 1089, 1069, 1010, 1256, 1462, 1603, 19745, 3795, 1659, 1055, 1050, 1057, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 17653, 13213, 56613, 3795, 1659, 1053, 1054, 1046, 1049, 1055, 1010, 1256, 1462, 1603, 103266, 3033, 3795, 1659, 1049, 1056, 1053, 1046, 1051, 1057, 1010, 1256, 1462, 1603, 54683, 56613, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 104008, 74980, 3795, 1659, 1051, 1055, 1046, 1052, 1052, 1010, 1256, 1462, 1603, 12791, 3795, 1603, 3932, 15913, 18580, 38546, 117608, 23600, 4456, 130282, 3448, 1051, 1046, 1603, 4638, 3795, 1032, 1049, 1049, 1047, 1048, 1052, 1047, 1050, 1048, 1050, 1049, 1010, 1256, 1462, 1603, 41676, 14044, 3795, 25773, 1461, 110611, 1010, 1256, 1462, 1603, 4271, 3795, 2628, 8481, 25698, 17631, 6758, 16622, 20388, 1079, 4614, 46078, 1010, 1256, 1462, 1603, 19745, 3795, 1659, 1052, 1052, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 17653, 13213, 56613, 3795, 1659, 1049, 1048, 1046, 1055, 1053, 1010, 1256, 1462, 1603, 103266, 3033, 3795, 1659, 1050, 1054, 1046, 1048, 1056, 1010, 1256, 1462, 1603, 54683, 56613, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 104008, 74980, 3795, 1659, 1055, 1046, 1049, 1055, 1267, 1052, 1046, 1603, 4638, 3795, 1032, 1049, 1049, 1047, 1048, 1052, 1047, 1050, 1048, 1050, 1049, 1010, 1256, 1462, 1603, 41676, 14044, 3795, 18755, 5136, 1384, 1461, 14846, 1800, 2928, 1010, 1256, 1462, 1603, 4271, 3795, 90583, 89399, 1086, 1032, 1049, 1048, 25285, 1045, 1054, 6758, 1032, 1050, 1052, 24205, 1439, 5193, 1010, 1256, 1462, 1603, 19745, 3795, 1659, 1049, 1051, 1056, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 17653, 13213, 56613, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 103266, 3033, 3795, 1659, 1049, 1051, 1056, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 54683, 56613, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 104008, 74980, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 12791, 3795, 1603, 116034, 1073, 7051, 20992, 117298, 5553, 33704, 3448, 1053, 1046, 1603, 4638, 3795, 1032, 1049, 1049, 1047, 1048, 1052, 1047, 1050, 1048, 1050, 1049, 1010, 1256, 1462, 1603, 41676, 14044, 3795, 25773, 1461, 110611, 1010, 1256, 1462, 1603, 4271, 3795, 2628, 8481, 25698, 17631, 6758, 16622, 20388, 1079, 4614, 46078, 1010, 1256, 1462, 1603, 19745, 3795, 1659, 1052, 1052, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 17653, 13213, 56613, 3795, 1659, 1049, 1048, 1046, 1055, 1053, 1010, 1256, 1462, 1603, 103266, 3033, 3795, 1659, 1050, 1054, 1046, 1048, 1056, 1010, 1256, 1462, 1603, 54683, 56613, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1256, 1462, 1603, 104008, 74980, 3795, 1659, 1055, 1046, 1049, 1055, 1267, 1438, 61253, 2364, 24185, 1045, 1603, 13458, 75586, 3795, 1659, 1055, 1054, 1054, 1046, 1048, 1048, 1010, 1045, 1603, 13458, 52735, 26464, 3033, 3795, 1659, 1049, 1051, 1051, 1046, 1056, 1052, 1010, 1045, 1603, 13458, 80239, 3033, 3795, 1659, 1053, 1052, 1050, 1046, 1057, 1052, 1010, 1045, 1603, 13458, 31550, 26464, 3033, 3795, 1659, 1048, 1046, 1048, 1048, 1010, 1045, 1603, 104008, 74980, 3795, 1659, 1056, 1057, 1046, 1050, 1050, 1267, 4380, 4440, 8222, 1261, 44433, 1307, 6591, 34207, 1044, 37597, 1562, 21834, 1044, 61349, 1044, 6482, 37597, 1044, 1321, 11829, 98283, 1394, 1278, 12581, 5451, 4521, 1046, 3730, 1395, 1261, 9134, 14655, 1455, 1925, 4810, 1486, 4108, 1317, 1261, 8705, 24628, 1044, 1321, 3866, 1486, 28982, 1536, 21834, 1046, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1732267318.280547, last_token_time=1732267318.280547, first_scheduled_time=1732267318.2864811, first_token_time=1732267318.700838, time_in_queue=0.005934238433837891, finished_time=1732267348.8584428, scheduler_time=0.10040408000349998, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "print(\"Processing images...\")\n",
    "for i, image_content in enumerate(content):\n",
    "    single_message = [{\"role\": \"user\", \"content\": [image_content]}]\n",
    "    try:\n",
    "        output = llm.chat(single_message, sampling_params=sampling_params)\n",
    "        print(f\"Debug: Raw output for Image {i + 1}:\\n{output}\\n\")  # Debug: Print raw output\n",
    "        outputs.append(output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Image {i + 1}: {e}\")\n",
    "        outputs.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a structured JSON from the outputs\n",
    "structured_data = []\n",
    "\n",
    "for idx, output in enumerate(outputs):\n",
    "    try:\n",
    "        if output:  # Ensure output is not None\n",
    "            # Adjust the extraction based on actual output structure\n",
    "            if isinstance(output, dict) and \"choices\" in output:\n",
    "                extracted_text = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "                structured_data.append({\n",
    "                    \"image\": f\"Image {idx + 1}\",\n",
    "                    \"text\": extracted_text.strip()\n",
    "                })\n",
    "            else:\n",
    "                structured_data.append({\n",
    "                    \"image\": f\"Image {idx + 1}\",\n",
    "                    \"text\": \"Unexpected output format\"\n",
    "                })\n",
    "        else:\n",
    "            structured_data.append({\n",
    "                \"image\": f\"Image {idx + 1}\",\n",
    "                \"text\": \"No output received\"\n",
    "            })\n",
    "    except (KeyError, IndexError, TypeError) as e:\n",
    "        structured_data.append({\n",
    "            \"image\": f\"Image {idx + 1}\",\n",
    "            \"error\": f\"Failed to extract text. Error: {e}\"\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structured JSON Output:\n",
      "[\n",
      "    {\n",
      "        \"image\": \"Image 1\",\n",
      "        \"text\": \"Unexpected output format\"\n",
      "    },\n",
      "    {\n",
      "        \"image\": \"Image 2\",\n",
      "        \"text\": \"Unexpected output format\"\n",
      "    }\n",
      "]\n",
      "\n",
      "JSON output has been saved to: /home/extracted_texts.json\n"
     ]
    }
   ],
   "source": [
    "# Convert the structured data to JSON string\n",
    "json_output = json.dumps(structured_data, indent=4)\n",
    "print(\"\\nStructured JSON Output:\")\n",
    "print(json_output)\n",
    "\n",
    "# Optionally save the JSON to a file\n",
    "output_file_path = \"/home/extracted_texts.json\"\n",
    "with open(output_file_path, \"w\") as json_file:\n",
    "    json_file.write(json_output)\n",
    "    print(f\"\\nJSON output has been saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
